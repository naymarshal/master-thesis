{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f76a8d2b-6f72-4dff-ac72-bc4194aa4097",
   "metadata": {},
   "source": [
    "**PREPARE FOR ML**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd19f1e3-27c7-4637-9e4e-caf3171dafd7",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a362a44d-8f52-44d1-90ee-d56676638ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer, pos_tag, word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f2305-c8ab-4f17-b1d3-32a7847fc9a1",
   "metadata": {},
   "source": [
    "Import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d249ece-786d-4253-a0d8-47e492a5bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to read csv file into dataset\n",
    "def read_csv_file(file): \n",
    "    dataset = pd.read_csv(file,header=0)\n",
    "    return dataset\n",
    "\n",
    "# Call funtion on file\n",
    "#df = read_csv_file(r\"C:\\Users\\nayma\\AppData\\Local\\Programs\\Python\\Python312\\Scripts\\thesis\\df_polarity.csv\")\n",
    "path = r\"C:\\Users\\jmarsal\\AppData\\Local\\anaconda3\\Scripts\\thesis\\df_train.csv\"\n",
    "df = read_csv_file(path)\n",
    "\n",
    "# Work only with relevant columns\n",
    "df = df[['post_id','comment_id','comment_text','polarity']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ce3f0-2327-4f31-a50e-76c789647f2f",
   "metadata": {},
   "source": [
    "Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b673c8-2e72-4df2-b0de-8311adeee88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of stop words\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "# List of missing stop words\n",
    "missing_stop_words = [\"i'm\", \"I'm\", 'im', 'Im','u', 'ur','youre','yall','yalls','the',\"don't\",'dont','doesnt']\n",
    "\n",
    "# Define functions to uppercase list and merge\n",
    "def up_and_merge(x):\n",
    "    x = merge_list(x, uppercase_list(x))\n",
    "    return x\n",
    "    \n",
    "def merge_list(x,*y):\n",
    "    for z in y:\n",
    "        x = x + z\n",
    "    return x\n",
    "\n",
    "def uppercase_list(x):\n",
    "    x_up = [i.upper() for i in x]\n",
    "    return x_up\n",
    "\n",
    "stop_words.extend(missing_stop_words)\n",
    "stop_words = up_and_merge(stop_words)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d924e09-ea8f-4c0d-926e-a22914b0fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to remove stopwords\n",
    "def remove_words(text_data:str,stop_words):\n",
    "    # Remove all words as specified in a custom list of words\n",
    "    return [item for item in text_data if item not in stop_words]\n",
    "\n",
    "# Define function to collapse to string\n",
    "def collapse_list_to_string(string_list):\n",
    "    # This is to join back together the text data into a single string\n",
    "    return ' '.join(string_list)\n",
    "\n",
    "\n",
    "# Remove stop words\n",
    "df['comment_train'] = df['comment_text'].astype(str).apply(lambda x: remove_words(x.split(),stop_words))\n",
    "df['comment_train'] = df['comment_train'].apply(collapse_list_to_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb7cf41-ff51-46cb-b2fb-052ae51d7ac7",
   "metadata": {},
   "source": [
    "Remove irrelevant punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c20b9d3-4cb7-4243-8644-65777b6a4907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant pucntuation and numbers, keep *!\n",
    "def remove_irrelevant_punctuation(text):\n",
    "    list_punct = [\"'\", \":\", \")\", \"(\", '\"', '`', '/', \".\", \"?\", \"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"+\", \"_\", \"~\", \",\", \"‚Äô\", \n",
    "                  \" üèæ\", \"<\", \">\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "    for i in list_punct:\n",
    "        text = text.replace(i, \"\")                \n",
    "    return text\n",
    "\n",
    "# Add space between words and ! \n",
    "def add_space(comment):\n",
    "    comment = ''.join(' ' + char + ' ' if char==\"!\" else char for char in comment).strip()\n",
    "    return comment\n",
    "\n",
    "def blank_spaces(comment):\n",
    "    comment = comment.replace(r' {2,}',' ')\n",
    "    comment = comment.strip()\n",
    "    return comment\n",
    "    \n",
    "    \n",
    "df['comment_train'] = df['comment_train'].apply(remove_irrelevant_punctuation)\n",
    "df['comment_train'] = df['comment_train'].apply(add_space)\n",
    "df['comment_train'] = df['comment_train'].apply(blank_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea57f4-db52-4dfb-9dc1-2633ebff36c5",
   "metadata": {},
   "source": [
    "Lemmatize (note this may not do uppers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee91777-0689-43c6-b8b9-27622b6c12ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define function to tag part-of-speech word if they are verbs or adjectives\n",
    "def pos(word):\n",
    "    pos_label = (pos_tag(word_tokenize(word))[0][1][0]).lower()\n",
    "    if pos_label == 'j': pos_label = 'a'  \n",
    "    if pos_label not in ['a', 's', 'v']: #if word is not an adjective or a verb, label as a noun\n",
    "        pos_label = 'n'\n",
    "    return pos_label\n",
    "\n",
    "# Define function to lemmatize words\n",
    "def lemm(text):\n",
    "    lemma = []\n",
    "    for t in text.split():\n",
    "        if t.isupper():\n",
    "            t = lemmatizer.lemmatize(t.lower(), pos(t.lower()))\n",
    "            lemma.append(t.upper())\n",
    "        else:\n",
    "            t = lemmatizer.lemmatize(t, pos(t))\n",
    "            lemma.append(t)\n",
    "    text = ' '.join(lemma)\n",
    "    return text\n",
    "        \n",
    "# Lemmatize words in comments\n",
    "df['comment_train'] = df['comment_train'].apply(lemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de4fffe-0373-4221-80a0-37923dcfc6d2",
   "metadata": {},
   "source": [
    "Save dataset to work on further notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d324d9-51f0-4f5b-9127-116c6b85ffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\jmarsal\\AppData\\Local\\anaconda3\\Scripts\\thesis\\df_to_train.csv\"\n",
    "df.to_csv(path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
